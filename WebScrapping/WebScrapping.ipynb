{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48af2daf-0122-4aea-9155-06f512155160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9687dd-b793-4566-8a73-ddbc06360a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Page 1\n",
      "Scraped Page 2\n",
      "Scraped Page 3\n",
      "Scraped Page 4\n",
      "Scraped Page 5\n",
      "Scraped Page 6\n",
      "Scraped Page 7\n",
      "Scraped Page 8\n",
      "Scraped Page 9\n",
      "Scraped Page 10\n",
      "Scraping completed. 100 quotes saved to 'all_quotes_dynamic.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# store all quotes\n",
    "quotes_data = []\n",
    "\n",
    "# URL\n",
    "base_url = 'https://quotes.toscrape.com/page/'\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    # URL\n",
    "    url = f'{base_url}{page}/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract all quotes on the page\n",
    "    quote_blocks = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    # end the loop\n",
    "    if not quote_blocks:\n",
    "        print(\"No more quotes found.\")\n",
    "        break\n",
    "    \n",
    "    # Extract quote text, author, and tags\n",
    "    for quote in quote_blocks:\n",
    "        text = quote.find('span', class_='text').get_text(strip=True)\n",
    "        author = quote.find('small', class_='author').get_text(strip=True)\n",
    "        tags = [tag.get_text(strip=True) for tag in quote.find_all('a', class_='tag')]\n",
    "        \n",
    "        quotes_data.append({\n",
    "            'Author': author,\n",
    "            'Quote': text,\n",
    "            'Tags': ', '.join(tags)\n",
    "        })\n",
    "    \n",
    "    print(f\"Scraped Page {page}\")\n",
    "    \n",
    "    # Check for \"Next\" button\n",
    "    next_button = soup.find('li', class_='next')\n",
    "    if next_button:\n",
    "        page += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(quotes_data)\n",
    "df.to_csv('Cleaned_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Scraping completed. {len(df)} quotes saved to 'all_quotes_dynamic.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
